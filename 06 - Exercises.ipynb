{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a93ab7f",
   "metadata": {},
   "source": [
    "### Q1\n",
    "\n",
    "Load the `mnist.npz` data file to variable `D`.   \n",
    "Then, pick out the labels and images of \"0\" and \"1\". Assign them the variables `lbls` and `imgs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "922e4388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12665\n",
      "(12665, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# add your code below\n",
    "D = np.load(\"C:/Users/HP/OneDrive/Documents/Study/AI/AIclass/mnist.npz\")\n",
    "trn_lbls = D[\"train_labels\"]\n",
    "trn_imgs = D[\"train_imgs\"]\n",
    "idx = (trn_lbls == 0) | (trn_lbls == 1)\n",
    "lbls = trn_lbls[idx]\n",
    "imgs = trn_imgs[idx]\n",
    "# add your code above\n",
    "if all([v in globals() for v in ['D', 'lbls', 'imgs']]):\n",
    "    print(lbls.size)  # this should be 12665\n",
    "    print(imgs.shape)  # this should be (12665, 28, 28)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75a7a03",
   "metadata": {},
   "source": [
    "### Q2\n",
    "\n",
    "We'll stick to the same two features: pixel area and convex hull.\n",
    "\n",
    "Calculate the two features following the lecture slides, and assign them to the variables `areas` and `hulls`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e4737744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96.4\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "from skimage import morphology\n",
    "# add your code below\n",
    "sz = lbls.size\n",
    "hulls = np.zeros(sz)\n",
    "areas = np.zeros(sz)\n",
    "for n in range(sz):\n",
    "    bw = imgs[n] < 128\n",
    "    areas[n] = np.sum(bw)\n",
    "    bw = morphology.convex_hull_image(imgs[n] < 128)\n",
    "    hulls[n] == np.sum(bw)\n",
    "\n",
    "\n",
    "\n",
    "# add your code above\n",
    "if all([v in globals() for v in ['areas', 'hulls']]):\n",
    "    print(np.round(np.mean(areas), 1))  # this should be 96.4\n",
    "    print(np.round(np.mean(hulls), 1))  # this should be 158.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9e14fc",
   "metadata": {},
   "source": [
    "### Q3\n",
    "\n",
    "Normalize both features by subtracting their mean and dividing by the standard deviation. Assign the normalized features to the variables `z_areas` and `z_hulls`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1ae02bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# add your code above\n",
    "if all([v in globals() for v in ['z_areas', 'z_hulls']]):\n",
    "    print(int(np.sum(np.abs(z_areas))))  # this should be 10928\n",
    "    print(int(np.sum(np.abs(z_hulls))))  # this should be 11878"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d705bb29",
   "metadata": {},
   "source": [
    "### Q4\n",
    "\n",
    "Create the machine learning model function, named `model`. It should have 3 input arguments: `w` (weights), `b` (bias), and `x` (features).\n",
    "\n",
    "This time, instead of the logistic function, use the **hyperbolic tangent (tanh) activation function** instead. The tanh activation function will also transform the input to fit the 0.0 to 1.0 probability bounds. The formula is as follows:\n",
    "\n",
    "$$y = w \\cdot x + b$$\n",
    "$$y_{prob} = \\frac{e^y - e^{-y}}{2 \\cdot (e^y + e^{-y})} + 0.5$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a273a4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# add your code above\n",
    "if 'model' in locals():\n",
    "    tmp = np.array([-3, 0.75, 4])\n",
    "    print(np.round(np.sum(model(0.5, 0.3, tmp)), 3))  # this should be 1.867"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ebe6fed",
   "metadata": {},
   "source": [
    "### Q5\n",
    "\n",
    "Now add a loss function, named `loss`. It should have 2 input arguments: `lbls` (gold standard labels), and `preds` (prediction probabilities).\n",
    "\n",
    "This time, use **mean absolute error**. The formula for mean absolute error is as follows:\n",
    "\n",
    "$$loss = \\frac{1}{n} \\sum{\\left| y_{gstd}-y_{pred} \\right|}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17a7bd14",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# add your code above\n",
    "if 'loss' in locals():\n",
    "    tmp1, tmp2 = np.array([0, 1]), np.array([0.3, 0.8])\n",
    "    print(np.round(loss(tmp1, tmp2), 3))  # this should be 0.25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f313fe3a",
   "metadata": {},
   "source": [
    "### Q6\n",
    "\n",
    "Although we have two features, let's just optimize one, `z_hulls`.\n",
    "\n",
    "Referring to the lecture notes, write the code to optimize `z_hulls` using gradient descent, given the starting conditions below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bed19e81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "w, b = 1, 0\n",
    "delta = 1e-9\n",
    "lrn_rate = 1e-1\n",
    "losses = np.zeros(1000)\n",
    "# add your code below\n",
    "\n",
    "\n",
    "\n",
    "# add your code above\n",
    "print(np.round(losses[-1], 3))  # this should be around 0.029"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6d0711",
   "metadata": {},
   "source": [
    "### Q7\n",
    "\n",
    "Calculate the accuracy (in %) of the optimized model, and assign the accuracy value to the variable `acc`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7faf416c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# add your code above\n",
    "if 'acc' in locals():\n",
    "    print(np.round(acc, 1))  # this should be around 98.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b798861b",
   "metadata": {},
   "source": [
    "### Q8\n",
    "\n",
    "In **Q6**, the training loop ran 1000 times to find the optimal parameters for `w` and `b`. But if you plot `losses`, you should notice that the optimization seems to hit a plateau long before 1000 iterations. I.e., perhaps we don't *need* to run all 1000 iterations.\n",
    "\n",
    "Copy your code from **Q6** here, but modify it so that it **stops training** (exits the loop) once the improvement in the loss value between subsequent iterations is below `1e-4`.   \n",
    "I.e., in the loop, compare the current loss value to the loss value of the previous iteration. If the improvement (difference) is less than 1e-4, then exit the loop.\n",
    "\n",
    "NOTE: please use the variable `n` as your loop counter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb225b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "w, b = 1, 0\n",
    "delta = 1e-9\n",
    "lrn_rate = 1e-1\n",
    "losses = np.zeros(1000)\n",
    "# add your code below\n",
    "\n",
    "\n",
    "\n",
    "# add your code above\n",
    "if 'n' in locals():\n",
    "    print(n)  # this should be around 256"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Belajarpython",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
